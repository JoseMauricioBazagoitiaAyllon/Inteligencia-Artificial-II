{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP1Señales_trancito.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1EQ-FLAljtEFE1UJmr2faIURzEfNn7HMJ",
      "authorship_tag": "ABX9TyPnBME6yn7roNgK/XFtH0xZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoseMauricioBazagoitiaAyllon/Inteligencia-Artificial-II/blob/main/MLP1Se%C3%B1ales_trancito.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "_cdpUxrlO741"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"./drive/MyDrive/IA2/train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygocUDD2PEci",
        "outputId": "03249413-656e-4c37-935d-6ee38d6fc04b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00000_00024.jpg',\n",
              " '00000_00023.jpg',\n",
              " '00000_00004 - copia - copia.jpg',\n",
              " '00000_00004 - copia (2).jpg',\n",
              " '00001',\n",
              " '00000']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path =\"./drive/MyDrive/IA2/train\""
      ],
      "metadata": {
        "id": "soJvlTilPT6D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_transforms = transforms.Compose([\n",
        "                                          transforms.Resize((30,30)),\n",
        "                                          transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "NQCUYlLQPWa4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(root = train_dataset_path, transform =training_transforms)"
      ],
      "metadata": {
        "id": "LJanqS9iPs6T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset= train_dataset, batch_size = 32, shuffle= False)"
      ],
      "metadata": {
        "id": "hOvxHFBNQHCK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_and_std(loader):\n",
        "  mean = 0.\n",
        "  std = 0.\n",
        "  total_image_count = 0\n",
        "  for images,  _ in loader:\n",
        "    image_count_in_a_batch = images.size(0)  \n",
        "    print(images.shape)\n",
        "    images = images.view(image_count_in_a_batch, images.size(1),-1)\n",
        "    print(images.shape)\n",
        "    mean += images.mean(2).sum(0)\n",
        "    std +=images.std(2).sum(0)\n",
        "    total_image_count += image_count_in_a_batch\n",
        "  mean/= total_image_count\n",
        "  std /= total_image_count\n",
        "  return mean, std"
      ],
      "metadata": {
        "id": "ZAMCiEIFQg_9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_mean_and_std(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Digi_LJRuth",
        "outputId": "95b8be8b-34b5-4755-cf87-58c2126ed185"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 3, 30, 30])\n",
            "torch.Size([20, 3, 900])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.5632, 0.4982, 0.5180]), tensor([0.1337, 0.1292, 0.1415]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qyKaMIamIAho"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import os \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path =\"./drive/MyDrive/IA2/train\"\n",
        "test_dataset_path =\"./drive/MyDrive/IA2/test\""
      ],
      "metadata": {
        "id": "ItlBFcT2JmEI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.5632, 0.4982, 0.5180]\n",
        "std = [0.1337, 0.1292, 0.1415]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.Resize((30,30)),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.RandomRotation(10),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(torch.Tensor(mean),torch.tensor(std))            \n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "                                      transforms.Resize((30,30)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(torch.Tensor(mean),torch.tensor(std)) \n",
        "                                       \n",
        "])"
      ],
      "metadata": {
        "id": "9sWaoh1cLHCS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(root = train_dataset_path, transform =training_transforms) \n",
        "#test_dataset = torchvision.datasets.ImageFolder(root = test_dataset_path, transform =test_transforms) "
      ],
      "metadata": {
        "id": "VAWrFIseTCyR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9ZnIdx-Uan4",
        "outputId": "bbba5b14-575f-41e5-dbc7-9a171469d2ec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 20\n",
              "    Root location: ./drive/MyDrive/IA2/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(30, 30), interpolation=bilinear, max_size=None, antialias=None)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_Q1fqMahUzsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HGftM7EqUz0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)"
      ],
      "metadata": {
        "id": "32Q35dgaUz4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crossentropy(output, target):\n",
        "    logits = output[np.arange(len(output)),target]\n",
        "    entropy = - logits + np.log(np.sum(np.exp(output),axis=-1))\n",
        "    return entropy\n",
        "\n",
        "def grad_crossentropy(output, target):\n",
        "    answers = np.zeros_like(output)\n",
        "    answers[np.arange(len(output)),target] = 1    \n",
        "    return (- answers + softmax(output)) / output.shape[0]"
      ],
      "metadata": {
        "id": "JiuaaudtU2kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron():\n",
        "  def __init__(self, inputs, outputs):\n",
        "    self.w = np.random.normal(loc=0.0, \n",
        "          scale = np.sqrt(2/(inputs+outputs)), \n",
        "          size = (inputs, outputs)) \n",
        "    self.ws = []\n",
        "    self.loss = crossentropy\n",
        "    self.grad_loss = grad_crossentropy\n",
        "    \n",
        "  def __call__(self, w, x):\n",
        "    return np.dot(x, w) \n",
        "\n",
        "  def fit(self, x, y, epochs, lr, verbose=True, log_each=1):\n",
        "    x = np.c_[np.ones(len(x)), x]\n",
        "    for epoch in range(1,epochs+1):\n",
        "        # Batch Gradient Descent\n",
        "        y_hat = self(self.w, x)  \n",
        "        # función de pérdida\n",
        "        l = self.loss(y_hat, y).mean()\n",
        "        # derivadas\n",
        "        dldh = self.grad_loss(y_hat, y)\n",
        "        dhdw = x\n",
        "        dldw = np.dot(dhdw.T, dldh)\n",
        "        # actualizar pesos\n",
        "        self.w = self.w - lr*dldw\n",
        "        # guardar pesos para animación\n",
        "        self.ws.append(self.w.copy())\n",
        "        # print loss\n",
        "        if verbose and not epoch % log_each:\n",
        "            print(f\"Epoch {epoch}/{epochs} Loss {l}\")\n",
        "            \n",
        "  def predict(self, x):\n",
        "    x = np.c_[np.ones(len(x)), x]\n",
        "    return self(self.w, x)"
      ],
      "metadata": {
        "id": "GKXPs7LtU5MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = Perceptron(870, 2)\n",
        "epochs, lr = 200, 1"
      ],
      "metadata": {
        "id": "7nRA40BwU6HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron.fit(X_train, y_train, epochs, lr, log_each=10)"
      ],
      "metadata": {
        "id": "boGJipvuU9sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def reluPrime(x):\n",
        "  return x > 0"
      ],
      "metadata": {
        "id": "VnnubUC1U-bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def mse(y, y_hat):\n",
        "    return 0.5*np.mean((y_hat - y.reshape(y_hat.shape))**2)\n",
        "\n",
        "def grad_mse(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)"
      ],
      "metadata": {
        "id": "B7ABX9VlVBJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP():\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(D_in+H)),\n",
        "                                  size=(D_in, H)), np.zeros(H)\n",
        "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(H+D_out)),\n",
        "                                  size=(H, D_out)), np.zeros(D_out)\n",
        "    self.ws = []\n",
        "    self.loss = mse\n",
        "    self.grad_loss = grad_mse\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
        "    self.h = relu(self.h_pre)\n",
        "    y_hat = np.dot(self.h, self.w2) + self.b2 \n",
        "    return y_hat\n",
        "    \n",
        "  def fit(self, X, Y, epochs = 100, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
        "    batch_size = len(X) if batch_size == None else batch_size\n",
        "    batches = len(X) // batch_size\n",
        "    l = []\n",
        "    for e in range(1,epochs+1):     \n",
        "        # Mini-Batch Gradient Descent\n",
        "        _l = []\n",
        "        for b in range(batches):\n",
        "            x = X[b*batch_size:(b+1)*batch_size]\n",
        "            y = Y[b*batch_size:(b+1)*batch_size]        \n",
        "            y_pred = self(x) \n",
        "            loss = self.loss(y, y_pred)\n",
        "            _l.append(loss)        \n",
        "            # Backprop \n",
        "            dldy = self.grad_loss(y, y_pred) \n",
        "            grad_w2 = np.dot(self.h.T, dldy)\n",
        "            grad_b2 = dldy.mean(axis=0)\n",
        "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)      \n",
        "            grad_w1 = np.dot(x.T, dldh)\n",
        "            grad_b1 = dldh.mean(axis=0)\n",
        "            # Update (GD)\n",
        "            self.w1 = self.w1 - lr * grad_w1\n",
        "            self.b1 = self.b1 - lr * grad_b1\n",
        "            self.w2 = self.w2 - lr * grad_w2\n",
        "            self.b2 = self.b2 - lr * grad_b2\n",
        "        l.append(np.mean(_l))\n",
        "        self.ws.append((\n",
        "            self.w1.copy(),\n",
        "            self.b1.copy(),\n",
        "            self.w2.copy(),\n",
        "            self.b2.copy()\n",
        "        ))\n",
        "        if verbose and not e % log_each:\n",
        "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
        "\n",
        "  def predict(self, ws, x):\n",
        "    w1, b1, w2, b2 = ws\n",
        "    h = relu(np.dot(x, w1) + b1)\n",
        "    y_hat = np.dot(h, w2) + b2\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "UgaJQnEKVB8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(D_in=1, H=3, D_out=1)\n",
        "epochs, lr = 50, 0.0002\n",
        "model.fit(data_train, 0, epochs, lr, batch_size=1, log_each=10)"
      ],
      "metadata": {
        "id": "IPwUV6xeVF23"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}